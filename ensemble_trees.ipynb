{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Oct 29 15:40:04 2025\n",
    "\n",
    "@author: zemsk\n",
    "\n",
    "\n",
    "Bleaching Presence Detection\n",
    "Target variable: Percent_Bleaching\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "\n",
    "\n",
    "path = \"./coral-reef-global-bleaching\"\n",
    "filename_read = os.path.join(path, \"coral_whole.csv\")\n",
    "\n",
    "df = pd.read_csv(filename_read)\n",
    "\n",
    "# Removed only locations and labels, no real data touched\n",
    "# df.drop(\"Ocean_Name\", axis=1, inplace=True)\n",
    "# df.drop(\"Country_Name\", axis=1, inplace=True)\n",
    "# df.drop(\"Sample_ID\", axis=1, inplace=True)\n",
    "# df.drop(\"Date_Year\", axis=1, inplace=True)\n",
    "# df.drop(\"Bleaching_Level\", axis=1, inplace=True)\n",
    "# df.drop(\"Realm_Name\", axis=1, inplace=True)\n",
    "# # #Percent_Cover is not a best predictor and also contain 30% of its fields as null.\n",
    "# # # For the sake of bigger dataset this feature is dropped\n",
    "# df.drop(\"Percent_Cover\", axis=1, inplace=True)\n",
    "# # df.drop(\"ClimSST\", inplace=True, axis=1)\n",
    "# df.drop(\"Exposure\", inplace=True, axis=1)\n",
    "# # df.drop(\"Temperature_Maximum\", inplace=True, axis=1)\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "# df[\"Exposure\"] = label_encoder.fit_transform(df[\"Exposure\"])\n",
    "\n",
    "# These features were taken into account that data is nonlinear\n",
    "# df = df[[\"Distance_to_Shore\", \"Temperature_Mean\", \"Turbidity\", \"TSA\", \"Depth_m\", \"Percent_Bleaching\"]]\n",
    "\n",
    "# These features were taken into account that data is linear\n",
    "df = df[['Cyclone_Frequency', 'Depth_m', 'ClimSST', 'Distance_to_Shore', 'Turbidity', 'TSA', 'Temperature_Mean', 'Percent_Bleaching']]\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df.info()\n",
    "\n"
   ],
   "id": "1ca8cd0106f07dfe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=['Percent_Bleaching'])\n",
    "y = df['Percent_Bleaching']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ],
   "id": "cda8714a93d8a66d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import cuml\n",
    "import cudf\n",
    "from cuml.ensemble import RandomForestRegressor as cuRF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Convert data to cudf DataFrame (GPU equivalent of pandas DataFrame)\n",
    "X_cudf = cudf.DataFrame.from_records(X)\n",
    "y_cudf = cudf.Series(y)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cudf, y_cudf, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the CuML RandomForestRegressor (on GPU)\n",
    "rf_model = cuRF(n_estimators=600, max_depth=30, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test.to_array(), y_pred.to_array()))\n",
    "print(f'RMSE: {rmse}')\n"
   ],
   "id": "530da84f81966dcc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=600,\n",
    "    max_depth=30,\n",
    "    criterion=\"friedman_mse\",\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.,\n",
    "    bootstrap=False,\n",
    "\n",
    "    random_state=42,\n",
    "    n_jobs=-1)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ],
   "id": "c1422a0a8b7346a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "score = r2_score(y_test, y_pred)\n",
    "\n",
    "print(score)\n",
    "# print(model.oob_score_)\n",
    "print(mean_absolute_error(y_test, y_pred))\n"
   ],
   "id": "a1bcfdd4c3fadf6e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "I have a feeling that ~0.65 (max 0.687) is the best performance for RandomForest. No matte how I would tackle its params or change feature set. Results tent to improve slightly, when include more features, despite big multicollinearity. Mb we could look for some more features from big file.\n",
    "Could also try to take big file and apply PCA and see what happens"
   ],
   "id": "318076e022ab153"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, PredefinedSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data into DMatrix format (for GPU acceleration)\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "params = {\n",
    "    \"max_depth\": [12],  # Depth of trees\n",
    "    \"min_child_weight\": [1, 3, 6],  # Minimum sum of instance weight (hessian) in a child\n",
    "    \"gamma\": [0.3],  # Minimum loss reduction to make a further partition\n",
    "    \"subsample\": [0.7, 0.8, 1.0],  # Fraction of samples to grow each tree\n",
    "    \"colsample_bytree\": [0.5, 0.7, 1.0],  # Fraction of features to consider for each tree\n",
    "    \"reg_alpha\": [0, 0.1, 1],  # L1 regularization\n",
    "    \"reg_lambda\": [1, 10, 50],  # L2 regularization\n",
    "    \"learning_rate\": [0.05, 0.1],  # Step size shrinkage\n",
    "}\n",
    "\n",
    "# Add GPU-specific parameters to the param grid\n",
    "params_gpu = {\n",
    "    'tree_method': ['hist'],  # Use GPU-based histogram method for faster training\n",
    "    'device': ['cuda'],  # Specify GPU device (use CUDA)\n",
    "}\n",
    "\n",
    "# Combine the two parameter grids into one\n",
    "combined_params = {**params, **params_gpu}\n",
    "\n",
    "test_fold = np.concatenate([ -1 * np.ones(len(X_train)), 0 * np.ones(len(X_test)) ])\n",
    "\n",
    "\n",
    "ps = PredefinedSplit(test_fold)\n",
    "\n",
    "# Create a GridSearchCV object with the combined parameters\n",
    "gs = GridSearchCV(\n",
    "    xgb.XGBRegressor(),\n",
    "    combined_params,\n",
    "    cv=ps,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the model using GridSearchCV with GPU\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters from GridSearchCV\n",
    "print(\"Best parameters from GridSearchCV:\", gs.best_params_)\n",
    "\n",
    "# Get the best model from GridSearchCV\n",
    "best_model = gs.best_estimator_\n",
    "\n",
    "# Make predictions using the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate RMSE (Root Mean Squared Error)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f'Root Mean Squared Error: {rmse}')\n"
   ],
   "id": "20cf1a2c17ac7cc2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "evallist = [(dvalidation, 'validation'), (dtrain, 'train')]\n",
    "num_round = 50 # Number of boosting rounds\n",
    "\n",
    "bst = xgb.train(params, dtrain, num_round, evallist)\n",
    "print(\"XGBoost model training complete on GPU.\")"
   ],
   "id": "41b8d7036bd4131b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import xgboost as xgb\n",
    " \n",
    " \n",
    "\n",
    "params = {\n",
    "    'colsample_bytree': 1.0, \n",
    "    'device': 'cuda', \n",
    "    'gamma': 0.3, \n",
    "    'learning_rate': 0.1, \n",
    "    'max_depth': 12, \n",
    "    'min_child_weight': 6, \n",
    "    'reg_alpha': 0.1, \n",
    "    'reg_lambda': 1, \n",
    "    'subsample': 0.7, \n",
    "    'tree_method': 'hist',\n",
    "    'objective': 'reg:squarederror',  # For regression task\n",
    "    'eval_metric': 'mae',  \n",
    "}\n",
    "\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(**params)\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "bst = xgb.train(\n",
    "    params_gpu,\n",
    "    dtrain,\n",
    "    num_boost_round=600,\n",
    "    evals=[(dtrain, 'train')],\n",
    "    verbose_eval=True\n",
    ")"
   ],
   "id": "7368c1c6f345b425"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "params_gpu = {\n",
    "    'tree_method': ['hist'],\n",
    "    'device': ['cuda'],\n",
    "}\n",
    "\n",
    "params = {\n",
    "    \"max_depth\": [12],\n",
    "    \"min_child_weight\": [1, 3, 6],\n",
    "    \"gamma\": [0.3],\n",
    "    \"subsample\": [0.7, 0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.5, 0.7, 1.0],\n",
    "    \"reg_alpha\": [0, 0.1, 1],\n",
    "    \"reg_lambda\": [1, 10, 50],\n",
    "    \"learning_rate\": [0.05, 0.1],\n",
    "}\n",
    "\n",
    "combined_params = {**params, **params_gpu}\n",
    "\n",
    "test_fold = np.concatenate([\n",
    "    -1 * np.ones(len(X_train)),\n",
    "     0 * np.ones(len(X_test))\n",
    "])\n",
    "ps = PredefinedSplit(test_fold)\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    xgb.XGBRegressor(),\n",
    "    combined_params,\n",
    "    cv=ps,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "X_all = np.concatenate([X_train, X_test])\n",
    "y_all = np.concatenate([y_train, y_test])\n",
    "\n",
    "gs.fit(X_all, y_all)\n",
    "print(gs.best_params_)\n"
   ],
   "id": "eb9ba3ca8bc26448"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "score = r2_score(y_test, y_pred)\n",
    "\n",
    "print(score)\n",
    "# print(model.oob_score_)\n",
    "print(mean_absolute_error(y_test, y_pred))\n"
   ],
   "id": "e7afa73047142f49"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Compared to the Random Forest model, XGBoost improved predictive performance by 39%. Since I didnt observe strong nonlinear patterns during preprocessing, this gain likely comes from XGBoost ability to capture more complex feature interactions rather than nonlinear effects in individual variables. This is consistent with the correlation analysis: the Spearman matrix indicates that changes in some features influence others quite noticeably, while the mutual information matrix shows little evidence of direct nonlinear relationships. Taken together, it suggests that the datasetâ€™s complexity comes primarily from interactions between features rather than from standalone nonlinearities.",
   "id": "c1a042816accf134"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from matplotlib.pyplot import imread\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Saturday Nov 15 15:34 2025\n",
    "\n",
    "@author: 100yearsahead\n",
    "\n",
    "\n",
    "Bleaching Presence Detection\n",
    "Target variable: Percent_Bleaching\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "\n",
    "\n",
    "path = \"./coral-reef-global-bleaching\"\n",
    "filename_read = os.path.join(path, \"coral_whole.csv\")\n",
    "\n",
    "df = pd.read_csv(filename_read)\n",
    "\n",
    "# Removed only locations and labels, no real data touched\n",
    "# df.drop(\"Ocean_Name\", axis=1, inplace=True)\n",
    "# df.drop(\"Country_Name\", axis=1, inplace=True)\n",
    "# df.drop(\"Sample_ID\", axis=1, inplace=True)\n",
    "# df.drop(\"Date_Year\", axis=1, inplace=True)\n",
    "# df.drop(\"Bleaching_Level\", axis=1, inplace=True)\n",
    "# df.drop(\"Realm_Name\", axis=1, inplace=True)\n",
    "# # #Percent_Cover is not a best predictor and also contain 30% of its fields as null.\n",
    "# # # For the sake of bigger dataset this feature is dropped\n",
    "# df.drop(\"Percent_Cover\", axis=1, inplace=True)\n",
    "# # df.drop(\"ClimSST\", inplace=True, axis=1)\n",
    "# df.drop(\"Exposure\", inplace=True, axis=1)\n",
    "# # df.drop(\"Temperature_Maximum\", inplace=True, axis=1)\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "# df[\"Exposure\"] = label_encoder.fit_transform(df[\"Exposure\"])\n",
    "\n",
    "# These features were taken into account that data is nonlinear\n",
    "# df = df[[\"Distance_to_Shore\", \"Temperature_Mean\", \"Turbidity\", \"TSA\", \"Depth_m\", \"Percent_Bleaching\"]]\n",
    "\n",
    "# These features were taken into account that data is linear\n",
    "#df = df[['Cyclone_Frequency', 'Depth_m', 'ClimSST', 'Distance_to_Shore', 'Turbidity', 'TSA', 'Temperature_Mean', 'Percent_Bleaching']]\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df  = df.drop(columns=['Sample_ID'])\n",
    "df.info()\n",
    "print(df)\n"
   ],
   "id": "473cda92a9b7d6d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Split first (no leakage)\n",
    "X = df.drop(columns=['Percent_Bleaching'])\n",
    "y = df['Percent_Bleaching']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Since we have categorical variables we need to seperate the numeric and the categorical variables\n",
    "cat_cols = ['Realm_Name','Ocean_Name','Country_Name','Exposure','Bleaching_Level']\n",
    "num_cols = [col for col in X.columns if col not in cat_cols]\n",
    "\n",
    "\n",
    "\n",
    "# We one_hot_encode the categorical features\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "X_train_cat = ohe.fit_transform(X_train[cat_cols])\n",
    "X_test_cat  = ohe.transform(X_test[cat_cols])\n",
    "\n",
    "\n",
    "# We scale the numeric features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_num = scaler.fit_transform(X_train[num_cols])\n",
    "X_test_num  = scaler.transform(X_test[num_cols])\n",
    "\n",
    "\n",
    "# Combine the categorical and numerical features\n",
    "X_train_processed = np.hstack([X_train_num, X_train_cat])\n",
    "X_test_processed  = np.hstack([X_test_num, X_test_cat])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "1cdd1d3b3a928054"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=9,\n",
    "    learning_rate=0.3,\n",
    "    min_child_weight=1,\n",
    "    gamma=0, #we dont need to add penalty since there`s enough samples I guess\n",
    "    subsample=1,\n",
    "    colsample_bytree=1,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=1,\n",
    "    objective='reg:squarederror',\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train_processed, y_train)"
   ],
   "id": "9c6d0f1325d29403"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "y_pred = xgb_model.predict(X_test_processed)\n",
    "\n",
    "score = r2_score(y_test, y_pred)\n",
    "\n",
    "print(score)\n",
    "# print(model.oob_score_)\n",
    "print(mean_squared_error(y_test, y_pred))\n"
   ],
   "id": "a083bfc2a38c20d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=600,\n",
    "    max_depth=30,\n",
    "    criterion=\"friedman_mse\",\n",
    "    max_features=0.5,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.,\n",
    "    bootstrap=False,\n",
    "    random_state=42,\n",
    "    n_jobs=-1)\n",
    "\n",
    "model.fit(X_train_processed, y_train)"
   ],
   "id": "89fb5d1dc0511f77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "y_pred = model.predict(X_test_processed)\n",
    "\n",
    "score = r2_score(y_test, y_pred)\n",
    "\n",
    "print(score)\n",
    "# print(model.oob_score_)\n",
    "print(mean_absolute_error(y_test, y_pred))\n"
   ],
   "id": "f9eb5e2ed3670454"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The good performance of the Random Forest model on high-dimensional feature sets can be attributed to its ensemble nature and its ability to aggregate a large number of weak but diverse predictors.\n",
    "\n",
    "In contrast, XGBoost rely on sequential learning, where each new tree attempts to correct the residual errors of the previous ones. When the dataset contains only a few strong predictors, XGBoost can explore these features more deeply and model complex interactions, often achieving superior predictive performance.\n",
    "\n",
    "Thus, Random Forests may outperform boosting methods in scenarios with large numbers of weak predictors, whereas XGBoost is m\n",
    "ore effective when the dataset contains fewer but more influential features."
   ],
   "id": "2b0f81c93b20b1e8"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
