{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "**Dataset:** *Coral reef global bleaching*\n",
    "\n",
    "\n",
    "**Objective:** *Predict reef bleaching*\n",
    "\n",
    "**Author:** *Danylo Zemskyi, Omari March*\n",
    "\n",
    "**Date:** *15/12/2025*\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### 1.1 Problem Statement\n",
    "\n",
    "* Description of the task: Bleaching prediction for Coral Reef Global Bleaching dataset (Population/Colony wide).\n",
    "* Target variable: This problem is about predicting how much coral bleaching occurs (measured as Percent Bleaching) based on environmental and physical factors like water depth, temperature, distance from shore, exposure, turbidity, and cyclone frequency (Regression problem).\n",
    "* Topic relevance: Coral reefs play a vital role in marine ecosystem health. Coral reefs are able to support diverse ecosystems, filter the water, and create oxygen when they have clean, clear, and cool water to support them. Also, they protect shorelines by reducing the impact from waves, storms, and floods. This helps prevent loss of life, property damage, and erosion. When reefs are damaged or destroyed, coastal communities can suffer increased damage from severe storms, and even normal wave action. Bleached corals continue to live, but they are more vulnerable to disease and starvation.\n",
    "\n",
    "### 1.2 Dataset Overview\n",
    "\n",
    "* Source of the dataset: https://www.kaggle.com/datasets/mehrdat/coral-reef-global-bleaching\n",
    "* Number of samples and features: The dataset consists of 2 csv files, which have 18 and 62 Features, and 41361 rows each. Csv file with 62 features provides a lot of redundant metrics that can be derived with formulas. Both files can be merged by Sample_ID column\n",
    "* Feature types (numerical, categorical, target)\n"
   ],
   "id": "76e0783baf60c42f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Experimental Setup\n",
    "\n",
    "### 2.1 Libraries\n",
    "\n",
    "Used libs\n",
    "- numpy\n",
    "- pandas\n",
    "\n",
    "\n",
    "\n",
    "- matplotlib\n",
    "- seaborn\n",
    "- os\n",
    "\n",
    "- sklearn\n",
    "- statsmodels\n",
    "- scipy\n",
    "\n",
    "\n",
    "- xgboost\n",
    "- tensorflow.keras\n",
    "### 2.2 Evaluation Metrics\n",
    "\n",
    "* Metrics used: R2 and MAE\n",
    "* Justification: MAE is preferred in tasks involving skewed or outlier-prone data because it provides a robust, scale-consistent measure of average model error without disproportionately penalizing large deviations.\n",
    "\tAs the baseline model mean predictor was chosen, which can give a sense what is the minimal mean absolute error model has to archive to be better than a simple mean model (MAEbaseline = 11.29).\n",
    "\tR2 provides an interpretable measure of overall model fit and how well the model captures data patterns relative to a naive mean-based predictor, which is picked as baseline performance.\n",
    "\n",
    "### 2.3 Dataset split\n",
    "\n",
    "* Split method used: train-test split(TTS)\n",
    "* TTS chosen over k-fold cross-validation as the dataset is sufficiently large to minimize overfitting risks associated with TTS, while significantly reducing computational cost by requiring only a single model training instead of multiple iterations.\n",
    "\n",
    "\n"
   ],
   "id": "d191292d09cce1fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\"\"\"Data manipulations libs\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"Plotting libs\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "\"\"\"Models evaluations imports\"\"\"\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, root_mean_squared_error\n",
    "\n",
    "\"\"\"Data preprocessing imports\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from scipy.stats import skew, boxcox\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "\n",
    "\"\"\"Dataset stats imports\"\"\"\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\"\"\"Models that were used imports\"\"\"\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dropout"
   ],
   "id": "ebbf754418058ed7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Baseline Model\n",
    "\n",
    "\n",
    "### 3.1 Baseline Definition\n",
    "\n",
    "Since this the task involves regression for Percent_Bleaching prediction, the baseline performance can be assessed using simple models such as the mean predictor or median predictor, which always predict the average or median bleaching percentage across the dataset. That will provide us with a reference level of performance to compare more complex models against.\n",
    "\n",
    "### 3.2 Baseline Results\n"
   ],
   "id": "55bc3e6dd22084c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "path = \"./coral-reef-global-bleaching\"\n",
    "filename_read = os.path.join(path, \"coral_whole.csv\")\n",
    "\n",
    "df = pd.read_csv(filename_read)\n",
    "\n",
    "df  = df.drop(columns=['Sample_ID', 'Percent_Cover', 'Date_Year']) #Percent_Cover has 30% of its row NA, so it significantly cuts dataset\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "df.info()\n",
    "print(df)"
   ],
   "id": "395dfd13596d0313"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "cat_cols = [col for col in df.columns if col not in num_cols]\n",
    "\n",
    "target_value_col = \"Percent_Bleaching\""
   ],
   "id": "ea67f06ef07ea627"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Target value\n",
    "y = df[target_value_col].to_list()\n",
    "\n",
    "# Mean prediction\n",
    "y_median = np.median(y)\n",
    "y_mean = np.mean(y)\n",
    "\n",
    "y_pred_median = np.full(len(y), y_median)\n",
    "y_pred_mean = np.full(len(y), y_mean)\n",
    "\n",
    "# Metrics Median\n",
    "mae_median = mean_absolute_error(y, y_pred_median)\n",
    "rmse_median = root_mean_squared_error(y, y_pred_median)\n",
    "r2_median = r2_score(y, y_pred_median)\n",
    "\n",
    "print(\"Median Baseline Results:\")\n",
    "print(f\"MAE: {mae_median:.4f}\")\n",
    "print(f\"RMSE: {rmse_median:.4f}\")\n",
    "print(f\"R: {r2_median:.4f}\")\n",
    "\n",
    "# Metrics Mean\n",
    "mae_mean = mean_absolute_error(y, y_pred_mean)\n",
    "rmse_mean = root_mean_squared_error(y, y_pred_mean)\n",
    "r2_mean = r2_score(y, y_pred_mean)\n",
    "\n",
    "print(\"\\nMean Baseline Results:\")\n",
    "print(f\"MAE: {mae_mean:.4f}\")\n",
    "print(f\"RMSE: {rmse_mean:.4f}\")\n",
    "print(f\"R: {r2_mean:.4f}\")\n",
    "\n"
   ],
   "id": "21b124b58b2beb4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Exploratory Data Analysis\n",
    "\n",
    "### 4.1 Target Distribution\n",
    "\n",
    "Histogram shows that target value has a lot of outliers and extremely skewed around 0%, which implies that Percent_Bleaching values has to be scaled to mitigate impact of imbalance distribution and normalized. Nevertheless, The histogram highlights a fundamental difference between population-level and colony-level bleaching observations. Population-level measurements provide a broad, aggregated indication of overall bleaching severity across reef sites, whereas colony-level observations capture finer-scale variability and extreme responses at the individual colony level. Because these two data types represent different ecological scales and statistical distributions, training a single model on both would conflate distinct processes and potentially bias predictions. Therefore, separate models are required to accurately capture bleaching dynamics at each observational scale."
   ],
   "id": "e32400821f4e93a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create boolean masks to separate samples with and without bleaching\n",
    "\n",
    "df_population = df[df[\"Bleaching_Level\"] != \"Colony\"]\n",
    "df_colony = df[df[\"Bleaching_Level\"] != \"Population\"]\n",
    "\n",
    "bins = np.arange(0, 105, 5)  # from 0 to 100 in steps of 0.5\n",
    "\n",
    "# Generate bin labels (not directly used in plotting)\n",
    "labels = [f\"{b}–{b+5}%\" for b in bins[:-1]]\n",
    "\n",
    "# Plot histogram for no bleaching samples\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(\n",
    "    df_population[target_value_col],\n",
    "    bins=bins,\n",
    "    color=\"orange\",\n",
    "    alpha=0.6,\n",
    "    edgecolor=None,\n",
    "    label=\"Population level\"\n",
    ")\n",
    "\n",
    "# Plot histogram for samples with bleaching\n",
    "plt.hist(\n",
    "    df_colony[target_value_col],\n",
    "    bins=bins,\n",
    "    color=\"blue\",\n",
    "    alpha=0.6,\n",
    "    edgecolor=None,\n",
    "    label=\"Colony Level\"\n",
    ")\n",
    "plt.xticks(bins)\n",
    "plt.xlabel(\"Percent Bleaching (%)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Histogram of Percent_Bleaching (Population Vs Colony)\")\n",
    "plt.grid(axis=\"y\", linestyle=\":\", linewidth=0.5)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "ce579ca1f7dd25c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.2 Feature Distribution and Skewness\n",
    "\n",
    "The majority of numerical features in the dataset exhibit pronounced skewness, with several key variables showing strong right-skew (e.g., Distance_to_Shore, Turbidity, Percent_Bleaching) and others displaying left-skewed distributions (e.g., ClimSST, Temperature_Mean, TSA), indicating non-normal feature distributions and further motivating the use of transformation and non-linear modeling approaches.\n"
   ],
   "id": "ec56a0f151543396"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def get_skewness(df):\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    skewness_results = {}\n",
    "\n",
    "    # Iterate through numerical columns\n",
    "    for col in numeric_cols:\n",
    "        numeric_data = df[col]\n",
    "\n",
    "          # Compute skewness only if the column contains data\n",
    "        if len(numeric_data) > 0:\n",
    "            skew_val = skew(numeric_data)\n",
    "\n",
    "            # Store skewness value and categorize its type\n",
    "            skewness_results[col] = {\n",
    "                'skewness': skew_val,\n",
    "                'skew_type': 'Right (Positive)' if skew_val > 0.5 else 'Left (Negative)' if skew_val < -0.5 else 'Approximately Symmetric',\n",
    "\n",
    "            }\n",
    "\n",
    "    # Convert and sort results to a DataFrame for easier inspection\n",
    "    skewness_df = pd.DataFrame(skewness_results).T\n",
    "    skewness_df = skewness_df.sort_values('skewness', key=abs, ascending=False)\n",
    "\n",
    "    print(skewness_df[:-1])\n",
    "    return skewness_results\n",
    "\n",
    "\n",
    "skewness_results = get_skewness(df)"
   ],
   "id": "232af260b9f3869f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.3 Feature Relationships\n",
    "\n",
    "The Pearson correlation matrix shows generally weak linear relationships among the environmental variables and between predictors and the target variable (Percent_Bleaching). Most correlation coefficients are close to zero, indicating limited linear dependence."
   ],
   "id": "347cbed5c303200f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "df_numeric = df[num_cols]\n",
    "\n",
    "pearson_corr_matrix = df_numeric.corr(method=\"pearson\")\n",
    "\n",
    "features = pearson_corr_matrix.index\n",
    "\n",
    "# Use Heatmap to visualize matrix\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.heatmap(pearson_corr_matrix, annot=True, cmap=\"coolwarm\")"
   ],
   "id": "354d1b093d2cb6ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The Spearman correlation matrix reveals stronger monotonic relationships than those observed with Pearson correlation, indicating that several variables are related in a non-linear but monotonic manner.",
   "id": "dc925d5663dba228"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "spearman_corr_matrix = df_numeric.corr(method=\"spearman\")\n",
    "features = spearman_corr_matrix.index\n",
    "\n",
    "# Use Heatmap to visualize matrix\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.heatmap(spearman_corr_matrix, annot=True, cmap=\"coolwarm\")"
   ],
   "id": "1531e78493118cfb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The VIF analysis reveals severe multicollinearity among temperature-related variables (Temperature_Mean and ClimSST), confirming strong feature redundancy and motivating the use  non-linear models that are less sensitive to collinearity.",
   "id": "cec61bc876c0a0c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "vif_data = pd.DataFrame()\n",
    "\n",
    "# Add numerical feature names\n",
    "vif_data[\"features\"] = num_cols\n",
    "\n",
    "# Compute VIF for each numerical feature by regressing it against all other features\n",
    "vif_data[\"vif\"] = [\n",
    "    variance_inflation_factor(df_numeric.values, i)\n",
    "    for i in range(len(num_cols))\n",
    "]\n",
    "\n",
    "vif_data\n"
   ],
   "id": "a0ab7d8e53c2d071"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Mutual information regression was used to quantify the non-linear dependency between each numerical feature and the target variable, Percent Bleaching. Unlike correlation-based measures, mutual information captures both linear and non-linear relationships without assuming monotonicity. The results indicate that Distance to Shore, Cyclone Frequency, and temperature-related variables exhibit the strongest non-linear associations with bleaching severity, while Depth shows a comparatively weaker influence. As expected, the target variable itself ranks highest, serving as a validation of the method.\n",
   "id": "30ecff429ce3e2d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mi_X = df_numeric.drop(target_value_col, axis=1)\n",
    "\n",
    "\n",
    "## Compute mutual information scores between each numerical feature and the target variable\n",
    "mi = mutual_info_regression(mi_X, y)\n",
    "mi_scores = pd.Series(mi, index=mi_X.columns).sort_values(ascending=False)\n",
    "\n",
    "mi_scores"
   ],
   "id": "73ba267ecdd4a12e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Overall, the dataset exhibits non-linear relationships, significant feature redundancy, skewed feature distributions, and interaction effects, pushing the use of scaled non-linear models and ensemble methods for robust prediction.\n",
   "id": "27b9ee698b56d262"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Data Preprocessing\n",
    "\n",
    "Before selecting feature set it is better to see the effect of categorical data on DecisionTreeRegressor(), since they carry a lot of environmental data"
   ],
   "id": "fabe2ff72ccfbefd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def test_model(model, X_test, y_test, plot_title = \"\"):\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print(f\"R2 score for {plot_title}: {r2}\")\n",
    "    print(f\"MAE score for {plot_title}: {mae}\")\n",
    "\n",
    "    residuals = y_test - y_pred.ravel()\n",
    "\n",
    "    # Plot residuals vs predicted\n",
    "    plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "    plt.axhline(0, color='red', linestyle='--')\n",
    "    plt.xlabel(\"Predicted values\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.title(f\"Residuals Plot for {plot_title}\")\n",
    "    plt.show()\n"
   ],
   "id": "4c0081de8f696503"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "path = \"./coral-reef-global-bleaching\"\n",
    "filename_read = os.path.join(path, \"coral_whole.csv\")\n",
    "\n",
    "df_tmp = pd.read_csv(filename_read)\n",
    "\n",
    "\n",
    "# Percent_Cover tells how much of a corals were observed. This is irrelevant predictor, so its removed\n",
    "df_population = df_tmp.drop(columns=['Sample_ID', 'Percent_Cover','Date_Year'])\n",
    "\n",
    "# Split Population samples from Colony\n",
    "df_population = df_population[df_population['Bleaching_Level'] != 'Colony']\n",
    "df_population.drop(\"Bleaching_Level\", axis=1, inplace=True)\n",
    "df_population.dropna(inplace=True)\n",
    "\n",
    "\n",
    "df_colony = df_tmp.drop(columns=['Sample_ID', 'Percent_Cover', 'Date_Year'])\n",
    "# Split Population samples from Colony\n",
    "df_colony = df_colony[df_colony['Bleaching_Level'] != 'Population']\n",
    "df_colony.drop(\"Bleaching_Level\", axis=1, inplace=True)\n",
    "\n",
    "df_colony.dropna(inplace=True)\n",
    "\n",
    "df_tmp.info()\n",
    "df_population.info()\n",
    "df_colony.info()"
   ],
   "id": "175bbbccf48b70d8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### 5.1 Handle N/A\n",
    "\n",
    "NEED TO BE DONE\n",
    "\n",
    "\n",
    "### 5.2 Encoding & Transformation and Feature Scaling\n",
    "\n",
    "To address the skewness observed in the numerical features, skewness-aware transformations were applied to create a normalized version of the dataset. Right-skewed variables were transformed using the Box–Cox transformation after ensuring strictly positive values with a small shift for zero-values, while left-skewed variables were normalized using the Yeo–Johnson power transformation, which supports negative values. All transformations were applied to a separate normalized dataset to preserve the original data, improving distributional symmetry and supporting more stable and effective training of scale-sensitive models.\n"
   ],
   "id": "7ee926274ad3fbd5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"The following function for applying normalization to dataframe was refactored by ChatGPT. Appendix 10.1.1 contains my original code and the modified version suggested by ChatGPT \"\"\"\n",
    "\n",
    "def normalize_df(df, skewness_results):\n",
    "\n",
    "    df_normalized = df.copy()\n",
    "\n",
    "    right_skewed = [\n",
    "        col for col, v in skewness_results.items()\n",
    "        if v[\"skewness\"] > 0.5 and col in df_normalized.columns\n",
    "    ]\n",
    "\n",
    "    left_skewed = [\n",
    "        col for col, v in skewness_results.items()\n",
    "        if v[\"skewness\"] < -0.5 and col in df_normalized.columns\n",
    "    ]\n",
    "\n",
    "    print(\"Left-skewed:\", left_skewed)\n",
    "    print(\"Right-skewed:\", right_skewed)\n",
    "\n",
    "    for col in right_skewed:\n",
    "        x = df_normalized[col]\n",
    "\n",
    "        # Skip if constant or all NaN\n",
    "        if x.nunique(dropna=True) <= 1:\n",
    "            continue\n",
    "\n",
    "        if (x <= 0).any():\n",
    "            pt = PowerTransformer(method=\"yeo-johnson\")\n",
    "            df_normalized[[col]] = pt.fit_transform(df_normalized[[col]])\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            non_null = x.notna()\n",
    "            df_normalized.loc[non_null, col], _ = boxcox(x[non_null])\n",
    "        except Exception:\n",
    "            # Fallback safety\n",
    "            pt = PowerTransformer(method=\"yeo-johnson\")\n",
    "            df_normalized[[col]] = pt.fit_transform(df_normalized[[col]])\n",
    "\n",
    "    for col in left_skewed:\n",
    "        if df_normalized[col].nunique(dropna=True) <= 1:\n",
    "            continue\n",
    "\n",
    "        pt = PowerTransformer(method=\"yeo-johnson\")\n",
    "        df_normalized[[col]] = pt.fit_transform(df_normalized[[col]])\n",
    "\n",
    "    return df_normalized\n"
   ],
   "id": "ab879bac7c5b21d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Compute skewness for numerical features in both dataset variants\n",
    "skewness_results_population = get_skewness(df_population)\n",
    "skewness_results_colony = get_skewness(df_colony)\n",
    "\n",
    "# Apply skewness-based normalization to each dataset\n",
    "df_normalized_population = normalize_df(df_population, skewness_results_population)\n",
    "df_normalized_colony = normalize_df(df_colony, skewness_results_colony)"
   ],
   "id": "ddc38356fc77f91e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Categorical and numerical features were processed separately: categorical variables were encoded using one-hot encoding to convert them into a numerical representation, while numerical features were standardized to zero mean and unit variance. Scaling was applied even after normalization to ensure comparable feature magnitudes, which is particularly important for distance-based and gradient-based models. Finally, the processed numerical and categorical features were combined into a single feature matrix used for model training and evaluation.",
   "id": "cf2a31e344e9b7bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_preprocessed_split_data(df):\n",
    "    # Determine usable columns directly from the dataset\n",
    "\n",
    "    # Split first (no leakage)\n",
    "    X = df.drop(columns=[target_value_col])\n",
    "    y = df[target_value_col]\n",
    "\n",
    "\n",
    "    num_used = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_used = [col for col in X.columns if col not in num_used]\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "\n",
    "# We one_hot_encode the categorical features\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "\n",
    "    X_train_cat = ohe.fit_transform(X_train[cat_used])\n",
    "    X_test_cat  = ohe.transform(X_test[cat_used])\n",
    "    X_cat = ohe.transform(X[cat_used])\n",
    "\n",
    "    ohe_feature_names = ohe.get_feature_names_out(cat_used)\n",
    "\n",
    "    # We scale the numeric features\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Even after normalization its still crucial to scale-up data\n",
    "    X_train_num = scaler.fit_transform(X_train[num_used])\n",
    "    X_test_num  = scaler.transform(X_test[num_used])\n",
    "    X_num = scaler.transform(X[num_used])\n",
    "\n",
    "\n",
    "    # Combine the categorical and numerical features\n",
    "    X_train_processed = np.hstack([X_train_num, X_train_cat])\n",
    "    X_test_processed  = np.hstack([X_test_num, X_test_cat])\n",
    "    X_processed = np.hstack([X_num, X_cat])\n",
    "\n",
    "    # Store final processed feature names in correct order\n",
    "    processed_feature_names = np.concatenate([num_used, ohe_feature_names])\n",
    "\n",
    "    # Return processed data and metadata as dictionary\n",
    "    return {\n",
    "    \"X_processed\": X_processed,\n",
    "    \"y\": y,\n",
    "    \"X_train\": X_train_processed,\n",
    "    \"X_test\": X_test_processed,\n",
    "    \"y_train\": y_train,\n",
    "    \"y_test\": y_test,\n",
    "    \"feature_names\": processed_feature_names\n",
    "}"
   ],
   "id": "7d971a181bc54eb5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data_population = get_preprocessed_split_data(df_normalized_population)\n",
    "data_colony = get_preprocessed_split_data(df_normalized_colony)"
   ],
   "id": "6ee47a203f57e3eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 6. Support Vector Machines (SVM)\n",
    "\n",
    "### 6.1 Motivation for SVM\n",
    "\n",
    "Support Vector Machines (SVM) were initially selected due to their conceptual simplicity, strong theoretical grounding, and flexibility in modelling both linear and non-linear relationships through a variety of kernel functions. This versatility makes SVMs powerful in many structured prediction tasks.\n",
    "\n",
    "### 6.2 Kernel Experiments\n",
    "\n",
    "#### 6.2.1 RBF Kernel\n",
    "\n",
    "\n"
   ],
   "id": "45167aaf5935160"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_svr_population = SVR(kernel=\"rbf\", shrinking=True, tol=0.001)\n",
    "model_svr_colony = SVR(kernel=\"rbf\", shrinking=True, tol=0.001)\n",
    "\n",
    "model_svr_population.fit(data_population[\"X_train\"], data_population[\"y_train\"])\n",
    "model_svr_colony.fit(data_colony[\"X_train\"], data_colony[\"y_train\"])"
   ],
   "id": "673231ede0043219"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "test_model(model_svr_population, data_population[\"X_test\"], data_population[\"y_test\"], \"SVM (kernel = rbf) Population samples\")",
   "id": "ff6f98a9d1c0a693"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "test_model(model_svr_colony, data_colony[\"X_test\"], data_colony[\"y_test\"], \"SVM (kernel = rbf) Colony samples\")",
   "id": "40cbb14351a43abf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 6.2.2 Polynomial Kernel (degree 2)\n",
    "\n"
   ],
   "id": "85dc29505cbf6ae7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_svr_population = SVR(kernel=\"poly\", degree=2, shrinking=True, tol=0.001)\n",
    "model_svr_colony = SVR(kernel=\"poly\", degree=2, shrinking=True, tol=0.001)\n",
    "\n",
    "model_svr_population.fit(data_population[\"X_train\"], data_population[\"y_train\"])\n",
    "model_svr_colony.fit(data_colony[\"X_train\"], data_colony[\"y_train\"])"
   ],
   "id": "de4c1837db13d983"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "test_model(model_svr_population, data_population[\"X_test\"], data_population[\"y_test\"], \"SVM (kernel = poly, degree 2) Population samples\")",
   "id": "4d1f298797f2fcb0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "test_model(model_svr_colony, data_colony[\"X_test\"], data_colony[\"y_test\"], \"SMV (kernel = poly, degree 2) Colony samples\")",
   "id": "62b0f00aedf2d80e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 6.2.3 Polynomial Kernel (degree 3)",
   "id": "1b26b555825d0e46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_svr_population = SVR(kernel=\"poly\", degree=3, shrinking=True, tol=0.001)\n",
    "model_svr_colony = SVR(kernel=\"poly\", degree=3, shrinking=True, tol=0.001)\n",
    "\n",
    "model_svr_population.fit(data_population[\"X_train\"], data_population[\"y_train\"])\n",
    "model_svr_colony.fit(data_colony[\"X_train\"], data_colony[\"y_train\"])"
   ],
   "id": "b7517b200a6032c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "test_model(model_svr_population, data_population[\"X_test\"], data_population[\"y_test\"], \"SVM (kernel = poly, degree = 3) Population samples\")",
   "id": "2e7a22845d9ea6b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "test_model(model_svr_colony, data_colony[\"X_test\"], data_colony[\"y_test\"], \"SVM (kernel = poly, degree = 3) Colony samples\")",
   "id": "891660906669ad96"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6.3 SVM Results Comparison\n",
    "\n",
    " SVMs showed persistent scalability limitations when applied to our dataset. Training non-linear kernels was computationally expensive, making large-scale tuning impractical.\n",
    "\n",
    "Additionally, SVMs are sensitive to multicollinearity, which can distort the margin geometry and lead to unstable predictions. For this reason, only some experiments were selected for SVMs.\n",
    "\n",
    "\n"
   ],
   "id": "d4f3a63bea58d47b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Ensemble Tree Models\n",
    "\n",
    "A feature importance analysis was conducted using a Decision Tree Regressor to quantify the contribution of each predictor to the model’s performance. Importance scores were aggregated at the base-feature level to account for one-hot encoded categorical variables, and features exceeding a predefined importance threshold were selected as the most influential predictors\n"
   ],
   "id": "c73c86551a2f6a7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_base_feature(feature_name):\n",
    "    for col in list(num_cols) + list(cat_cols):\n",
    "        # Handles one-hot encoded names like Country_Name_Australia\n",
    "        if feature_name == col or feature_name.startswith(col + \"_\"):\n",
    "            return col\n",
    "    return feature_name  # fallback\n",
    "\n",
    "\n",
    "def tree_feature_importance(X, y, processed_feature_names, threshold=0.01, model=None):\n",
    "\n",
    "    # If no tree based model is provided, train a default Decision Tree regressor\n",
    "    if model is None:\n",
    "        model = DecisionTreeRegressor()\n",
    "        model.fit(X, y)\n",
    "\n",
    "    importance = model.feature_importances_\n",
    "\n",
    "    # Create a DataFrame pairing each processed feature with its importance score and sort by\n",
    "    importance_df = pd.DataFrame({\n",
    "        \"feature\": processed_feature_names,\n",
    "        \"importance\": importance\n",
    "    }).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "    # Map each processed (possibly one-hot encoded) feature back to its base feature\n",
    "    importance_df[\"base_feature\"] = importance_df[\"feature\"].apply(get_base_feature)\n",
    "\n",
    "    # Aggregate importance scores by base feature\n",
    "    # This sums contributions from all one-hot encoded levels of categorical variables\n",
    "    grouped_importance = (\n",
    "        importance_df\n",
    "        .groupby(\"base_feature\")[\"importance\"]\n",
    "        .sum()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    # Select base features whose aggregated importance exceeds the given threshold\n",
    "    selected_features_tree = [\n",
    "        f for f, s in zip(grouped_importance.index, grouped_importance.values)\n",
    "        if s > threshold\n",
    "    ]\n",
    "\n",
    "    # Print selected features and full aggregated importance table for inspection\n",
    "    print(f\"Selected features for trees: {selected_features_tree}\")\n",
    "    print(grouped_importance)\n",
    "\n",
    "    # Append the target variable so the returned dataset remains model-ready\n",
    "    selected_features_tree.append(target_value_col)\n",
    "\n",
    "    return selected_features_tree\n"
   ],
   "id": "f65cb56f2d25b3d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "selected_features_tree_population = tree_feature_importance(data_population[\"X_processed\"], data_population[\"y\"], data_population[\"feature_names\"])\n",
    "selected_features_tree_colony = tree_feature_importance(data_colony[\"X_processed\"], data_colony[\"y\"], data_colony[\"feature_names\"])"
   ],
   "id": "37fb2cc781d94a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 7.1 Model Selection\n",
    "\n",
    "Random Forest and XGBoost were selected as advanced ensemble learning methods to evaluate their capacity to model complex, non-linear relationships within the dataset. Both methods provide robust predictive performance, strong generalization, and built-in mechanisms to handle feature interactions, making them well-suited for heterogeneous environmental datasets. Their ability to capture non-linear patterns without extensive feature engineering motivated their use after simpler baseline models demonstrated limited effectiveness.\n"
   ],
   "id": "56548c71ba4f9e6b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7.2 Initial Ensemble Training",
   "id": "25cb2eef5c90f9ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tree_data_population = get_preprocessed_split_data(df_normalized_population[selected_features_tree_population])\n",
    "tree_data_colony = get_preprocessed_split_data(df_normalized_colony[selected_features_tree_colony])"
   ],
   "id": "7321fd9727d1a3f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**XGBoost**\n",
    "\n",
    "Hyperparameter tuning focused on the most influential parameters controlling model complexity and learning dynamics (n_estimators, max_depth, learning_rate). These parameters were prioritized due to their strong effect on bias–variance trade-offs and their ability to model nonlinear relationships in skewed target distributions.\n",
    "\n",
    "All other hyperparameters were fixed at default or standard values. The minimum child weight and gamma were kept low to allow sufficient tree growth given the available sample size, while subsampling and column sampling were disabled to leverage the full dataset. Regularization was limited to reg_lambda=1, and the squared error objective was used for continuous target prediction."
   ],
   "id": "46a90e242d0f2e6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "xgb_model_population = xgb.XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=9,\n",
    "    learning_rate=0.3,\n",
    "    min_child_weight=1,\n",
    "    gamma=0, # No need to add penalty since there is enough samples\n",
    "    subsample=1,\n",
    "    colsample_bytree=1,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=1,\n",
    "    objective='reg:squarederror',\n",
    ")\n",
    "xgb_model_colony = xgb.XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=9,\n",
    "    learning_rate=0.3,\n",
    "    min_child_weight=1,\n",
    "    gamma=0, # No need to add penalty since there is enough samples\n",
    "    subsample=1,\n",
    "    colsample_bytree=1,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=1,\n",
    "    objective='reg:squarederror',\n",
    ")\n",
    "\n",
    "\n",
    "xgb_model_population.fit(tree_data_population[\"X_train\"], tree_data_population[\"y_train\"])\n",
    "xgb_model_colony.fit(tree_data_colony[\"X_train\"], tree_data_colony[\"y_train\"])"
   ],
   "id": "ce09a3477f319fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "test_model(xgb_model_population, tree_data_population[\"X_test\"], tree_data_population[\"y_test\"], \"XGBoost Population samples\")",
   "id": "7538d152be90c8b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "test_model(xgb_model_colony, tree_data_colony[\"X_test\"], tree_data_colony[\"y_test\"], \"XGBoost Colony samples\")",
   "id": "edcb09fd8392d8f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Random Forest**\n",
    "\n",
    "Hyperparameter tuning focused on n_estimators and max_depth, as these parameters primarily control model stability, variance reduction, and the ability to capture complex nonlinear relationships. A relatively large forest with deep trees was selected to ensure robust averaging, particularly important for heterogeneous and sparse observations such as colony-level data.\n",
    "\n",
    "Other hyperparameters were fixed to standard or default values. The Friedman MSE criterion was used due to its ability to scale up well for large, while max_features=0.5 pushed feature diversity across trees. Minimum split and leaf sizes were kept low to allow more selective partitions, and bootstrapping was disabled to utilize the full dataset per tree."
   ],
   "id": "37457af5bd179061"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "rf_model_population = RandomForestRegressor(\n",
    "    n_estimators=600,\n",
    "    max_depth=30,\n",
    "    criterion=\"friedman_mse\",\n",
    "    max_features=0.5,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.,\n",
    "    bootstrap=False,\n",
    "    random_state=42,\n",
    "    n_jobs=-1)\n",
    "\n",
    "rf_model_colony = RandomForestRegressor(\n",
    "    n_estimators=600,\n",
    "    max_depth=30,\n",
    "    criterion=\"friedman_mse\",\n",
    "    max_features=0.5,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.,\n",
    "    bootstrap=False,\n",
    "    random_state=42,\n",
    "    n_jobs=-1)\n",
    "\n",
    "rf_model_population.fit(tree_data_population[\"X_train\"], tree_data_population[\"y_train\"])\n",
    "rf_model_colony.fit(tree_data_colony[\"X_train\"], tree_data_colony[\"y_train\"])"
   ],
   "id": "cd8dca228d77870"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "test_model(rf_model_population, tree_data_population[\"X_test\"], tree_data_population[\"y_test\"], \"Randon Forest Population samples\")",
   "id": "72478679a4d284ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "test_model(rf_model_colony, tree_data_colony[\"X_test\"], tree_data_colony[\"y_test\"], \"Randon Forest Population samples\")",
   "id": "86b7718bf34ba119"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 7.4 Feature set refinement\n",
    "\n",
    "Further feature refinement was not performed, as the selected feature set already demonstrated strong performance."
   ],
   "id": "5c2155000962a904"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "refined_features_tree_population = tree_feature_importance(tree_data_population[\"X_processed\"], tree_data_population[\"y\"], tree_data_population[\"feature_names\"], threshold=0.02, model=rf_model_population)\n",
    "\n",
    "refined_features_tree_colony = tree_feature_importance(tree_data_colony[\"X_processed\"], tree_data_colony[\"y\"], tree_data_colony[\"feature_names\"], threshold=0.02, model=rf_model_colony)\n",
    "\n",
    "refined_tree_data_population = get_preprocessed_split_data(df_normalized_population[refined_features_tree_population])\n",
    "refined_tree_data_colony = get_preprocessed_split_data(df_normalized_colony[refined_features_tree_colony])"
   ],
   "id": "d5f65285427b956f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "refined_xgb_model_population = xgb.XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=9,\n",
    "    learning_rate=0.3,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,  # No need to add penalty since there is enough samples\n",
    "    subsample=1,\n",
    "    colsample_bytree=1,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=1,\n",
    "    objective='reg:squarederror',\n",
    ")\n",
    "refined_xgb_model_colony = xgb.XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=9,\n",
    "    learning_rate=0.3,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,  # No need to add penalty since there is enough samples\n",
    "    subsample=1,\n",
    "    colsample_bytree=1,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=1,\n",
    "    objective='reg:squarederror',\n",
    ")\n",
    "\n",
    "refined_xgb_model_population.fit(refined_tree_data_population[\"X_train\"], refined_tree_data_population[\"y_train\"])\n",
    "refined_xgb_model_colony.fit(refined_tree_data_colony[\"X_train\"], refined_tree_data_colony[\"y_train\"])\n"
   ],
   "id": "39e3dadd929466c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "test_model(refined_xgb_model_population, refined_tree_data_population[\"X_test\"], refined_tree_data_population[\"y_test\"], \"Refined XGBoost Population samples\")",
   "id": "507c611571ff4f0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "test_model(refined_xgb_model_colony, refined_tree_data_colony[\"X_test\"], refined_tree_data_colony[\"y_test\"], \"Refined XGBoost Colony samples\")",
   "id": "479abbaae3a26d0e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "refined_rf_model_population = RandomForestRegressor(\n",
    "    n_estimators=600,\n",
    "    max_depth=30,\n",
    "    criterion=\"friedman_mse\",\n",
    "    max_features=0.5,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.,\n",
    "    bootstrap=False,\n",
    "    random_state=42,\n",
    "    n_jobs=-1)\n",
    "\n",
    "refined_rf_model_colony = RandomForestRegressor(\n",
    "    n_estimators=600,\n",
    "    max_depth=30,\n",
    "    criterion=\"friedman_mse\",\n",
    "    max_features=0.5,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.,\n",
    "    bootstrap=False,\n",
    "    random_state=42,\n",
    "    n_jobs=-1)\n",
    "\n",
    "refined_rf_model_population.fit(refined_tree_data_population[\"X_train\"], refined_tree_data_population[\"y_train\"])\n",
    "refined_rf_model_colony.fit(refined_tree_data_colony[\"X_train\"], refined_tree_data_colony[\"y_train\"])"
   ],
   "id": "fd74e53c9cbdfbce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "test_model(refined_rf_model_population, refined_tree_data_population[\"X_test\"], refined_tree_data_population[\"y_test\"], \"Refined Randon Forest Population samples\")",
   "id": "fd44cb2a6e5be5c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "test_model(refined_rf_model_colony, refined_tree_data_colony[\"X_test\"], refined_tree_data_colony[\"y_test\"], \"Refined Randon Forest Population samples\")",
   "id": "9f51186fe5353836"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 7.5 Ensemble Results\n",
    "\n",
    "Random Forest demonstrated the most robust performance, achieving the lowest MAE and stable R2, especially in the presence of numerous high-bleaching outliers. Its architecture of aggregated decision trees made it less sensitive to noise and better able to approximate the real distribution of extreme bleaching values.\n",
    "\tXGBoost delivered similarly strong results and showed superior performance within the main cluster of bleaching values. It benefited more from feature engineering than Random Forest, improving significantly after removing weak or correlated predictors. However, XGBoost tended to make fewer but larger errors, making it slightly less reliable in the outlier-heavy tail of the target distribution.\n"
   ],
   "id": "9f2229276e5dc31d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 8. Deep Neural Networks (DNN)\n",
    "\n",
    "### 8.1 Motivation for Deep Neural Networks (DNN)\n",
    "\n",
    "* **Handle non-linear relationships:** DNNs can model complex, non-linear dependencies between environmental and geographic variables.\n",
    "* **Leverage high-dimensional interactions:** While DNNs can learn interactions automatically, explicit feature engineering (threshold-based, interaction, and group-wise features) improves their ability to capture key ecological patterns.\n",
    "\n",
    "### 8.2 Feature engineering for DNN\n",
    "\n",
    "These engineered features capture the joint effect of two variables on coral bleaching that might not be captured."
   ],
   "id": "60de47172eb4b76f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dnn_df_population = df_population.copy()\n",
    "dnn_df_colony = df_colony.copy()\n",
    "\n",
    "dnn_df_colony.info()"
   ],
   "id": "20216a9551543bab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Interaction Feature Engineering**\n",
    "\n",
    "To capture potential nonlinear relationships between predictors, pairwise interaction features were generated by multiplying selected combinations of variables. Interaction terms were created only when both constituent features were present in the dataset, ensuring robustness across different data subsets. This approach enables the model to account for joint effects that may not be captured by individual features alone."
   ],
   "id": "a8b29d4842966002"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_interaction_features(df, interactions):\n",
    "\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Loop over each specified pair of features\n",
    "    for a, b in interactions:\n",
    "\n",
    "        # Ensure both features exist in the DataFrame\n",
    "        if a in df_copy.columns and b in df_copy.columns:\n",
    "            # Create an interaction feature by multiplying the two columns\n",
    "            df_copy[f\"{a}_x_{b}\"] = df_copy[a] * df_copy[b]\n",
    "\n",
    "    # Return the DataFrame with added interaction features\n",
    "    return df_copy"
   ],
   "id": "508f69ed89cdb489"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "interactions = [\n",
    "    # Thermal stress vs physical conditions\n",
    "    (\"TSA\", \"Depth_m\"),             # Shallow reefs may bleach more under thermal stress\n",
    "    (\"TSA\", \"Distance_to_Shore\"),   # Coastal reefs may respond differently to thermal anomalies\n",
    "\n",
    "    # Water clarity & stress\n",
    "    (\"Turbidity\", \"Temperature_Mean\"), # Murky water + high temps reduce photosynthesis\n",
    "    (\"Turbidity\", \"Distance_to_Shore\"),# Turbidity often higher nearshore\n",
    "\n",
    "\n",
    "    # Cyclones / physical disturbance\n",
    "    (\"Cyclone_Frequency\", \"Depth_m\"),           # Shallow reefs more impacted by storms\n",
    "    (\"Cyclone_Frequency\", \"Temperature_Mean\"),  # Heat stress + cyclone damage interaction\n",
    "    (\"Cyclone_Frequency\", \"Distance_to_Shore\"), # Coastal and open ocean effects\n",
    "\n",
    "    # Geographical conditions\n",
    "    (\"Depth_m\", \"Distance_to_Shore\"),   # Coastal shallow reefs behave differently\n",
    "]\n",
    "\n",
    "dnn_df_interaction_population = create_interaction_features(dnn_df_population, interactions)\n",
    "dnn_df_interaction_colony = create_interaction_features(dnn_df_colony, interactions)\n"
   ],
   "id": "e4b8efaeb80a7869"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Group-Based Feature Normalization and Category Consolidation**\n",
    "\n",
    "Group-based normalization was applied to selected numerical variables to account for systematic differences across categorical groupings. For each specified feature, values were standardized within groups defined by a reference categorical variable using z-score normalization, computed relative to the group mean and standard deviation. To prevent numerical instability, a small constant was added to the denominator."
   ],
   "id": "616ac9d922148299"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_country_distribution(df, title, min_n=100):\n",
    "    country_count = df[\"Country_Name\"].value_counts()\n",
    "    country_count = country_count[country_count >= min_n]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(country_count.index, country_count.values)\n",
    "    plt.xlabel(\"Country\")\n",
    "    plt.ylabel(\"Number of Samples\")\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.grid(axis=\"y\", linestyle=\":\", linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_country_distribution(dnn_df_colony, \"Sample Distribution Across Countries (Colony)\")\n",
    "plot_country_distribution(dnn_df_population, \"Sample Distribution Across Countries (Population)\")\n"
   ],
   "id": "41b6a4d49198b5b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def group_normalize(df, value_cols, group_col):\n",
    "\n",
    "    if target_value_col in value_cols:\n",
    "        value_cols.remove(target_value_col)\n",
    "\n",
    "\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Apply group-wise z-score normalization for each specified numerical column\n",
    "    for col in value_cols:\n",
    "        df_copy[f\"{col}_grp_z\"] = (\n",
    "            df_copy[col] -\n",
    "            df_copy.groupby(group_col)[col].transform(\"mean\")   # Subtract group mean\n",
    "        ) / (\n",
    "            df_copy.groupby(group_col)[col].transform(\"std\") + 1e-6 # Divide by group std\n",
    "        )\n",
    "\n",
    "    return df_copy\n"
   ],
   "id": "a6cdb5a99b7a3cff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def add_valid_groups(df, min_n=100):\n",
    "    # Create a copy of the DataFrame to avoid modifying the original data\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Identify countries with at least min_n observations\n",
    "    valid_countries = (\n",
    "        df_copy[\"Country_Name\"]\n",
    "        .value_counts()             # Count observations per country\n",
    "        .loc[lambda x: x >= min_n]  # Keep only sufficiently frequent countries\n",
    "        .index\n",
    "    )\n",
    "\n",
    "    # Assign infrequent countries to a common 'Other' category\n",
    "    df_copy[\"Country_group\"] = df_copy[\"Country_Name\"].where(\n",
    "        df_copy[\"Country_Name\"].isin(valid_countries),\n",
    "        \"Other\"\n",
    "    )\n",
    "\n",
    "    # Identify exposure categories with at least min_n observations\n",
    "    valid_exposure = (\n",
    "        df_copy[\"Exposure\"]\n",
    "        .value_counts()\n",
    "        .loc[lambda x: x >= min_n]  # Keep only sufficiently frequent categories\n",
    "        .index\n",
    "    )\n",
    "\n",
    "    # Assign infrequent exposure categories to a common 'Other' category\n",
    "    df_copy[\"Exposure_group\"] = df_copy[\"Exposure\"].where(\n",
    "        df_copy[\"Exposure\"].isin(valid_exposure),\n",
    "        \"Other\"\n",
    "    )\n",
    "\n",
    "    # Return the DataFrame with consolidated categorical groupings\n",
    "    return df_copy\n"
   ],
   "id": "c6831a4599100d61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # Select numeric columns (exclude target)\n",
    "\n",
    "dnn_df_groups_population = add_valid_groups(dnn_df_interaction_population, min_n=50)\n",
    "dnn_df_groups_colony = add_valid_groups(dnn_df_interaction_colony)\n",
    "\n",
    "dnn_num_cols_population = (\n",
    "    dnn_df_groups_population\n",
    "    .select_dtypes(include=[np.number])\n",
    "    .columns\n",
    "    .drop(\"Percent_Bleaching\")\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "dnn_num_cols_colony = (\n",
    "    dnn_df_groups_colony\n",
    "    .select_dtypes(include=[np.number])\n",
    "    .columns\n",
    "    .drop(\"Percent_Bleaching\")\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "dnn_df_groups_population = group_normalize(dnn_df_groups_population, value_cols=dnn_num_cols_population, group_col=\"Country_group\")\n",
    "dnn_df_groups_population = group_normalize(dnn_df_groups_population, value_cols=dnn_num_cols_population, group_col=\"Exposure_group\")\n",
    "\n",
    "\n",
    "dnn_df_groups_colony = group_normalize(dnn_df_groups_colony, value_cols=dnn_num_cols_colony, group_col=\"Country_group\")\n",
    "dnn_df_groups_colony = group_normalize(dnn_df_groups_colony, value_cols=dnn_num_cols_colony, group_col=\"Exposure_group\")\n",
    "\n",
    "\n",
    "dnn_df_groups_colony.info()"
   ],
   "id": "552cd4c59e17beb5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Compute skewness\n",
    "dnn_skewness_population = get_skewness(dnn_df_groups_population)\n",
    "dnn_skewness_colony = get_skewness(dnn_df_groups_colony)\n",
    "\n",
    "# Normalize data\n",
    "dnn_df_normalized_population = normalize_df(\n",
    "    dnn_df_groups_population, dnn_skewness_population\n",
    ")\n",
    "\n",
    "dnn_df_normalized_colony = normalize_df(\n",
    "    dnn_df_groups_colony, dnn_skewness_colony\n",
    ")\n",
    "\n",
    "dnn_df_normalized_colony.info()"
   ],
   "id": "e454199d8384f8df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "dnn_data_population = get_preprocessed_split_data(dnn_df_normalized_population)\n",
    "dnn_data_colony = get_preprocessed_split_data(dnn_df_normalized_colony)"
   ],
   "id": "e36928a6558f0767"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 8.3 Architecture Design\n",
    "\n",
    "The neural network uses two fully connected hidden layers with ReLU activation to model nonlinear relationships while maintaining moderate complexity. The number of hidden units was set using a two-thirds-of-input heuristic, which served as the baseline architecture. We evaluated alternative designs by varying network depth (2 vs. 3 layers), width (70 vs. 128 units), dropout placement, and learning rate.\n",
    "\n",
    "Results show that increasing capacity beyond the baseline produced target-dependent gains. For the colony task, wider (128–128) and deeper (128–64–32) architectures achieved lower MAE than the baseline, indicating benefits from additional capacity in modelling more heterogeneous patterns. In contrast, for the population task, the baseline architecture achieved the best generalisation, with deeper and wider models exhibiting higher error and reduced R², suggesting mild overfitting. Hidden-layer dropout consistently degraded performance across both targets, while moderate learning rates (≈0.002) converged most reliably.\n",
    "\n",
    "Overall, the baseline two-layer architecture provided the most consistent performance across both tasks, offering a robust balance between capacity and generalisation on skewed, high-dimensional data, and was therefore selected as the final model."
   ],
   "id": "b1313ec6bc7c2d44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_dnn(X):\n",
    "\n",
    "    nodes_number = int((X.shape[1] + 1) * 2/3)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape = (X.shape[1],)))\n",
    "    model.add(Dense(nodes_number, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(nodes_number, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    return model\n"
   ],
   "id": "52dd91ff9d8ffa3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_dnn_population = get_dnn(dnn_data_population[\"X_train\"])\n",
    "\n",
    "model_dnn_colony = get_dnn(dnn_data_colony[\"X_train\"])\n",
    "\n",
    "model_dnn_population.summary()\n",
    "model_dnn_colony.summary()"
   ],
   "id": "1c6835e89c1c60b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 8.4 Training & Optimization\n",
    "\n",
    "  Both neural network models were trained using the mean squared error (MSE) loss function and optimized with the Adam optimizer due to its adaptive learning rate and stable convergence properties. Model performance was monitored using mean absolute error (MAE). Early stopping was used with patience of 8 and 10 epochs and restoration of the best-performing weights to prevent overfitting."
   ],
   "id": "7c77203fa5062809"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "early_stop_population = EarlyStopping(\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    monitor='loss'\n",
    ")\n",
    "\n",
    "early_stop_colony = EarlyStopping(\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    monitor='loss'\n",
    ")\n",
    "\n",
    "model_dnn_population.compile(optimizer=\"adam\",\n",
    "              loss=\"mse\",\n",
    "              metrics=['mae'],\n",
    "               )\n",
    "\n",
    "model_dnn_colony.compile(optimizer=\"adam\",\n",
    "              loss=\"mse\",\n",
    "              metrics=['mae'])"
   ],
   "id": "54aeab56cf66975c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model_dnn_population.fit(dnn_data_population[\"X_train\"], dnn_data_population[\"y_train\"], epochs=200, verbose=2, callbacks=[early_stop_population])\n",
   "id": "d257c2fe0382cf5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model_dnn_colony.fit(dnn_data_colony[\"X_train\"], dnn_data_colony[\"y_train\"], epochs=200, verbose=2, callbacks=[early_stop_colony])",
   "id": "4ce91a0c08a7a057"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 9.5 DNN Performance\n",
   "id": "a6ca40ec4bed69e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "test_model(model_dnn_population, dnn_data_population[\"X_test\"], dnn_data_population[\"y_test\"], \"DNN Population Samples\")",
   "id": "91ca944d4305ad2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "test_model(model_dnn_colony, dnn_data_colony[\"X_test\"], dnn_data_colony[\"y_test\"], \"DNN Colony Samples\")",
   "id": "2bd6b6348d3ec57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def score_feature_importance_dnn(model, X_test, y_test, feature_names):\n",
    "\n",
    "    result = permutation_importance(\n",
    "        model,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        n_repeats=3,\n",
    "        random_state=42,\n",
    "        scoring=\"neg_mean_absolute_error\"\n",
    "    )\n",
    "\n",
    "    importance = result.importances_mean\n",
    "\n",
    "    # Feature importance\n",
    "    importance_df = pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"importance\": importance\n",
    "    }).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "    def get_base_feature(feature_name):\n",
    "        for col in list(num_cols) + list(cat_cols):\n",
    "            if feature_name.startswith(col):\n",
    "                return col\n",
    "        return feature_name.split(\"_\")[0]\n",
    "\n",
    "    importance_df[\"base_feature\"] = importance_df[\"feature\"].apply(get_base_feature)\n",
    "\n",
    "    grouped_importance = (\n",
    "        importance_df\n",
    "        .groupby(\"base_feature\")[\"importance\"]\n",
    "        .sum()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    grouped_importance = grouped_importance / grouped_importance.sum()\n",
    "\n",
    "    threshold = 0.02\n",
    "    selected_features = [\n",
    "        f for f, s in zip(grouped_importance.index, grouped_importance.values)\n",
    "        if s > threshold\n",
    "    ]\n",
    "    print(f\"Selected features for trees: {selected_features}\")\n",
    "    print(grouped_importance)"
   ],
   "id": "1fd3861d19c06918"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "score_feature_importance_dnn(model_dnn_population, dnn_data_population[\"X_test\"], dnn_data_population[\"y_test\"], dnn_data_population[\"feature_names\"])",
   "id": "eaced915da0fe2ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "score_feature_importance_dnn(model_dnn_colony, dnn_data_colony[\"X_test\"], dnn_data_colony[\"y_test\"], dnn_data_colony[\"feature_names\"])",
   "id": "a00f45edb1ebed7e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 9. Conclusions\n",
    "\n",
    "### 9.1. Final Model Selection\n",
    "\n",
    "- **Selected model**\n",
    "Ensemble tree-based models (Random Forest and XGBoosting) were selected as the final models.\n",
    "The dataset exhibits  non-linear relationships, complex interactions between environmental and geographic variables, and pronounced skewness with outliers. Ensemble tree methods handle these characteristics effectively, as they do not rely on distance-based metrics or gradient optimization that can be distorted by extreme values. In contrast, SVMs and deep neural networks showed higher sensitivity to feature scaling, skewness, and extreme observations, resulting in less stable performance.\n",
    "\n",
    "- **Strengths and weaknesses**\n",
    "The main strengths of ensemble trees include robustness to outliers, the ability to implicitly capture feature interactions without explicit feature engineering, and consistent generalization across different dataset variants. Their primary limitations are reduced interpretability compared to linear models and higher computational cost relative to simpler approaches, particularly when training large ensembles.\n",
    "\n",
    "### 9.2. Key Findings\n",
    "\n",
    "* Causal structure (DAG) for coral bleaching analysis:\n",
    "\n",
    "```text\n",
    "   Geography ---> Human Impact ---> Bleaching\n",
    "        |              |                ∧\n",
    "        |              ∨                |\n",
    "        --------> Environment -----------\n",
    "```\n",
    "\n",
    "   **Geography** (e.g., distance to shore, ocean, country, depth) affects:\n",
    "        - Human impact (population, fishing, pollution)\n",
    "        - Environmental conditions (temperature, turbidity, light, currents)\n",
    "        - Bleaching directly through natural ecological pathways\n",
    "\n",
    "   **Human impact** also influences the environment (e.g., pollution, sedimentation), creating indirect effects on bleaching.\n",
    "\n",
    "   To estimate the causal effect Human Impact -> Bleaching, geographic variables must be included as controls to block backdoor paths.\n",
    "\n",
    "  * Model Behavior: Support Vector Machines (SVM)\n",
    "\n",
    "    Support Vector Machines exhibited limited suitability for this task due to significant scalability constraints, with training becoming computationally prohibitive on larger datasets and a hyperparameter tuning was impractical. Model performance was further degraded by sensitivity to multicollinearity, as highly correlated predictors distorted the margin structure and led to unstable predictions. Additionally, the heavily skewed distribution of Percent_Bleaching posed challenges for SVMs, resulting in systematic underestimation of extreme bleaching events. However, the model demonstrated comparatively better performance on the colony-level dataset, where the target distribution was less skewed.\n",
    "\n",
    "\n",
    "  * Model Behavior: Ensemble Tree Models (Random Forest & XGBoost)\n",
    "    **Random Forest**\n",
    "\n",
    "    Random Forest demonstrated stronger performance on the colony-level dataset, which is characterized by sparser observations. By aggregating predictions across many decision trees, the model effectively averages local patterns and is robust to outliers, resulting in more frequent but smaller errors and lower MAE. Its performance remained stable even when weak or correlated features were removed, indicating low sensitivity to feature selection. However, training time increased with high-cardinality categorical variables, though excluding these features substantially improved computational efficiency.\n",
    "\n",
    "    **XGBoost**\n",
    "\n",
    "    XGBoost performed better on the population-level dataset, which exhibits a denser and more skewed distribution of bleaching values. The model showed strong predictive accuracy within the dominant data cluster (approximately 0–35% bleaching), highlighting its effectiveness in learning patterns from concentrated clusters of data.XGBoost produced fewer but larger errors, but making extreme bleaching predictions less accurate .It benefited more noticeably from feature selection and the removal of weak or correlated predictors compared to Random Forest.\n",
    "\n",
    " * Model Behavior: Deep Neural Network (DNN)\n",
    "\n",
    "    The Deep Neural Network achieved moderate performance following extensive feature engineering, particularly when interaction features were explicitly constructed to capture relationships between related variables. The model exhibited behavior partly similar to Support Vector Machines, showing sensitivity to feature representation and target distribution. However, unlike SVMs, the DNN did not suffer from scalability limitations, as its optimization in parameter space allowed efficient training on larger datasets.\n",
    "\n",
    "### 9.3 Limitations\n",
    "\n",
    "* Imbalanced data: The dataset is highly imbalanced, with a disproportionate number of non-bleaching reefs compared to fewer bleaching and severely bleached observations, which add bias toward majority patterns and makes accurate modeling of extreme bleaching events more challenging.\n",
    "\n",
    "* Multicollinearity: Temperature-related variables (Temperature_Mean, Temperature_Maximum, ClimSST) and other correlated features exhibit high variance inflation factors (VIFs), potentially destabilizing models that assume independent predictors.\n",
    "\n",
    "### 9.4 Future Improvements\n",
    "\n",
    " * Feature engineering: Some dataset features still exhibit extreme multicollinearity, which could be mitigated through better targeted feature selection, dimensionality reduction, or the construction of orthogonal representations. Space-based models will benefit this.\n",
    "\n",
    "* More data:\n",
    "    Scraping the data sources referenced in the dataset documentation to collect additional observations, improving coverage and model generalization for moderate and severe bleached reef.\n",
    "\n",
    "* Model tuning:\n",
    "    Exploit the temporal structure of the dataset by transitioning from feedforward DNNs to sequence models such as RNNs or LSTMs to better capture time-dependent bleaching dynamics. Also apply some feature engineering based on tree feature importance and permutations scores."
   ],
   "id": "2a9a855a62b3943a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 10. Appendix\n",
    "\n",
    "### 10.1 LLM prompts\n",
    "\n",
    "\n",
    "#### 10.1.1 normalize_df bug fix\n",
    "\n",
    "\n",
    "* Prompt and suggested refactoring for normalize_df function from ChatGPT\n",
    "\n",
    "Original Code:\n",
    "```\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew, boxcox\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "skewness_results = {}\n",
    "\n",
    "# Identify skew\n",
    "for col in numeric_cols:\n",
    "    clean_data = df[col].dropna()\n",
    "    if len(clean_data) > 0:\n",
    "        skew_val = skew(clean_data)\n",
    "        skewness_results[col] = {\n",
    "            'skewness': skew_val,\n",
    "            'skew_type': 'Right (Positive)' if skew_val > 0.5 else 'Left (Negative)' if skew_val < -0.5 else 'Approximately Symmetric',\n",
    "        }\n",
    "\n",
    "right_skewed = {col: data for col, data in skewness_results.items() if data['skewness'] > 0.5}\n",
    "left_skewed = {col: data for col, data in skewness_results.items() if data['skewness'] < -0.5}\n",
    "\n",
    "# Transform right-skewed columns with Box-Cox (requires all positive)\n",
    "for col in right_skewed.keys():\n",
    "    non_null_mask = df[col].notnull()\n",
    "    # Replace zeros with small positive value\n",
    "    df.loc[non_null_mask, col] = df.loc[non_null_mask, col].replace(0, 1e-6)\n",
    "    # Apply Box-Cox\n",
    "    df.loc[non_null_mask, col], lam = boxcox(df.loc[non_null_mask, col])\n",
    "\n",
    "# Transform left-skewed columns with Yeo-Johnson (handles negatives & zeros)\n",
    "for col in left_skewed.keys():\n",
    "    pt = PowerTransformer(method='yeo-johnson')\n",
    "    df[[col]] = pt.fit_transform(df[[col]])\n",
    "\n",
    "# Summary\n",
    "skewness_df = pd.DataFrame(skewness_results).T\n",
    "skewness_df = skewness_df.sort_values('skewness', key=abs, ascending=False)\n",
    "print(skewness_df[:-1])\n",
    "```\n",
    "\n",
    "Prompt:\n",
    "\n",
    "```\n",
    "ValueError                                Traceback (most recent call last)\n",
    "Cell In[14], line 28\n",
    "     26     df.loc[non_null_mask, col] = df.loc[non_null_mask, col].replace(0, 1e-6)\n",
    "     27     # Apply Box-Cox\n",
    "---> 28     df.loc[non_null_mask, col], lam = boxcox(df.loc[non_null_mask, col])\n",
    "     30 # Transform left-skewed columns with Yeo-Johnson (handles negatives & zeros)\n",
    "     31 for col in left_skewed.keys():\n",
    "\n",
    "File ~\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_morestats.py:1150, in boxcox(x, lmbda, alpha, optimizer)\n",
    "   1147     raise ValueError(\"Data must not be constant.\")\n",
    "   1149 if np.any(x <= 0):\n",
    "-> 1150     raise ValueError(\"Data must be positive.\")\n",
    "   1152 # If lmbda=None, find the lmbda that maximizes the log-likelihood function.\n",
    "   1153 lmax = boxcox_normmax(x, method='mle', optimizer=optimizer)\n",
    "\n",
    "ValueError: Data must be positive\n",
    "\n",
    "\n",
    "Help me to resolve this issue\n",
    "```\n",
    "\n",
    "Response\n",
    "\n",
    "```\n",
    "The error occurs because Box-Cox requires strictly positive, non-constant data, and some of your columns still violate this—even after replacing zeros. You can wrap the Box-Cox step in a try-except block to skip problematic columns and avoid stopping the whole process. Here’s a corrected version of your function:\n",
    "\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import numpy as np\n",
    "\n",
    "def normalize_df(df, skewness_results):\n",
    "    df_normalized = df.copy()\n",
    "\n",
    "    # Identify right- and left-skewed features\n",
    "    right_skewed = {col: data for col, data in skewness_results.items() if data['skewness'] > 0.5}\n",
    "    left_skewed  = {col: data for col, data in skewness_results.items() if data['skewness'] < -0.5}\n",
    "\n",
    "    print(\"Left-skewed:\", list(left_skewed.keys()))\n",
    "    print(\"Right-skewed:\", list(right_skewed.keys()))\n",
    "\n",
    "    # Box-Cox for right-skewed features (positive only)\n",
    "    for col_right in right_skewed.keys():\n",
    "        try:\n",
    "            col_data = df_normalized[col_right].copy()\n",
    "\n",
    "            # Skip constant columns\n",
    "            if col_data.nunique() <= 1:\n",
    "                print(f\"Skipping {col_right} (constant column)\")\n",
    "                continue\n",
    "\n",
    "            # Make strictly positive\n",
    "            min_val = col_data.min()\n",
    "            if min_val <= 0:\n",
    "                col_data = col_data + abs(min_val) + 1e-6\n",
    "\n",
    "            df_normalized[col_right], lam = boxcox(col_data)\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping {col_right} due to Box-Cox error: {e}\")\n",
    "\n",
    "    # Yeo-Johnson for left-skewed features (handles negatives & zeros)\n",
    "    for col_left in left_skewed.keys():\n",
    "        try:\n",
    "            pt = PowerTransformer(method='yeo-johnson')\n",
    "            df_normalized[[col_left]] = pt.fit_transform(df_normalized[[col_left]])\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {col_left} due to PowerTransformer error: {e}\")\n",
    "\n",
    "    return df_normalized\n",
    "\n",
    "# Apply normalization\n",
    "df = normalize_df(df, skewness_results)\n",
    "\n",
    "Key Improvements:\n",
    "\n",
    "Try-except around Box-Cox and Yeo-Johnson to catch errors.\n",
    "\n",
    "Skip constant columns.\n",
    "\n",
    "Automatically shift negative or zero values to positive for Box-Cox.\n",
    "\n",
    "Prints skipped columns with reason for debugging.\n",
    "\n",
    "This will make your normalization robust and won’t stop for any column errors.\n",
    "\n",
    "If you want, I can also merge Box-Cox and Yeo-Johnson automatically, so you don’t have to worry about which columns are positive—everything gets a safe transform. Do you want me to do that?\n",
    "```\n",
    "\n",
    "#### 10.1.2 Suggestion for code report structure\n",
    "Prompt:\n",
    "\n",
    "`give me a structure of a final code report for training models over some ds. A baseline model was trained first then ds was explored (Data distribution, Check for non linearity, check feature importance) and preproccessed, then some svm with scaled data were tested for different kernels, then some ensemble trees were build with refinements and feature importance adjustments, then dnn were trained with properly scaled data. it has to be a notebook fomat`"
   ],
   "id": "618dc2b1723ac23a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
